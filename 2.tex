\chapter{Título provisório do segundo capítulo}
É difícil ver o noticiário hoje sem se deparar com as Inteligências Artificiais (IAs).
Contudo, entre o uso de ChatGPT para trabalhos escolares, trabalhadores preocupados com perder o emprego para uma máquina, e até alguns com medo da humanidade ser inteiramente aniquilada num apocalipse da era digital, o uso de IAs é tão mais comum quanto mais dissimulado.

A culpa disso é do fato que ``Inteligência Artificial'' está muito mais próximo de uma expressão de marketing feita para atrair a atenção do que de um termo técnico.
Ela pressupõe uma semelhança entre o pensamento orgânico, tal como humanos o fazem, e um suposto pensamento sintético que, como há de ser visto nesse capítulo, é inteiramente dessemelhante a seu equivalente vivo.

Por trás da Inteligência Artificial há um conjunto de técnicas estatísticas e computacionais que fazem o chamado \textit{aprendizado de máquina} ou \textit{aprendizado estatístico}.
Antes de entender \textit{como} um computador pode aprender, é necessário responder \textit{por que} se quer que ele aprenda.
Há muito que um computador pode fazer sem a necessidade desse treinamento: isso inclui fazer contas, navegar pela Internet, assistir a um filme, tipografar uma monografia, entre outros.
Então, o que diferencia essas atividades de uma Inteligência Artificial?

A resposta é que essas atividades podem ser (e são) descritas por meio de código, o que requer representá-las como um conjunto de instruções claras e objetivas: uma receita de como as realizar.
É fácil elaborar uma receita de como reproduzir um vídeo, por exemplo.
O computador deverá localizar o arquivo que contém o vídeo, abri-lo, decodificá-lo e reproduzi-lo, convertendo as informações numéricas para cores a uma determinada taxa de vezes por segundo.
Por mais que cada passo aqui descrito possa ser desmembrado em tantos outros mais, qualquer programa de computador é descritível como um conjunto de instruções.
Todo processador é construído aos moldes de uma Arquitetura de Conjunto de Instruções (ISA, do inglês \textit{Instruction Set Architecture}), um modelo abstrato que delimita, simplificadamente, como se pode programá-lo.
Um dos componentes disso é um conjunto de instruções que o processador pode executar, para o qual \textit{todos} os programas que são usados devem ser reduzidos.
Essas instruções são coisas simples, como ``coloque tal valor em tal lugar'', ``some tal valor com tal valor'' e ``faça isso se tal valor for verdadeiro''.

É possível criar uma ``receita para escrever''?
Ou uma ``receita para recomendar vídeos''?
Isto é, um conjunto de instruções objetivas que permita realizar essas ações?
Não. 
E é aqui que entra o uso de uma IA: aproximar objetivamente processos que não podem ser descritos assim.

\section{Regressões Lineares}\label{sec:2:regres}
Observe o seguinte gráfico com o valor da anomalia da temperatura média do globo terrestre em relação à média do século 20, entre 1970 e 2025:

\begin{figure}[H]
    \centering
    \caption{Gráfico da anomalia (°C) em função do tempo (anos), 1970-2025}
    \includegraphics[width=\linewidth]{static/R/plot/anom-1-hist.png}
    \label{fig:2:anom-1-hist}
\end{figure}

Empiricamente, é possível perceber que existe uma tendência nesses dados, que seguem aproximadamente uma reta.
Porém, é difícil determinar precisamente de quanto é essa tendência, e para onde o gráfico continuará. 
Isso se deve ao fato de que o planeta Terra é um sistema complexo: ele não segue uma ordem do tipo ``aumente 0,5°C ao ano''.
Na metáfora da seção anterior, é impossível estabelecer uma receita para a temperatura do planeta em função do ano.

Pode-se, contudo, aproximá-la.
Sabe-se, a partir desses dados, como o modelo deve se comportar.
Não é possível, principalmente quando se trata de algo tão simples como uma reta, que ele abarque cada mínima mudança e inconsistência; é, afinal, uma aproximação.

Qual deverá ser a forma desse modelo?
Matematicamente, uma reta pode ser definida por meio de uma função linear, que, nesse caso, assumiria a forma de

\begin{equation}
A(t) = t\cdot\beta_1 + \beta_0
\end{equation}

Isto é, a anomalia $A$, em função do tempo $t$, pode ser definida como o tempo $t$ multiplicado por um coeficiente $\beta_1$ somado a um coeficiente estático $\beta_0$.
Assim, para determinar a anomalia, $t$ deve ser transformado, segundo os parâmetros $\beta_0$ e $\beta_1$ que, até agora, \textit{não se sabe quais são}.
Intuitivamente, $\beta_1$ é a inclinação da reta e $\beta_0$ a altura dela.
A grande pergunta, portanto, é quais são esses coeficientes, ou seja, como $t$ deve ser mudado para obter $A$.

Para determinar esses dois coeficientes, $\beta_0$ e $\beta_1$, é necessário primeiro conceber uma maneira de comparar dois modelos, e descobrir qual deles mais se aproxima do fenômeno aproximado.
Uma maneira muito simples de fazer isso é, para cada ano, contabilizar a distância entre o valor previsto pelo modelo e a observação real.
O objetivo da criação do modelo, assim, passa a ser reduzir ao máximo possível esse erro.
Comumente, ao invés de simplesmente usar o erro, é considerado o erro elevado ao quadrado; assim, erros menores são menos penalizados, e erros maiores serão ainda mais penalizados, pois seu valor crescerá com mais rapidez.
Essa abordagem é chamada de ``mínimos quadrados'', pois almeja minimizar os quadrados dos erros.

O processo de descobrir $\beta_0$ e $\beta_1$ é conhecido como \textit{aprendizado} ou \textit{treinamento}.
Ele envolve inicializar esses valores com um ``chute'', e, gradualmente, melhorá-los.
Para isso, muda-se ou $\beta_0$ ou $\beta_1$, incrementando-o por pouco. 
Se a mudança for bem-vinda, ou seja, diminuir o erro, ela é mantida; caso contrário, o original é melhor.
Ao repetir esse processo várias vezes, a reta será acomodada nos coeficientes com menor valor de erro.

No caso da figura \ref{fig:2:anom-1-hist}, isso toma a forma da figura \ref{fig:2:anom-3-scatter+reg}.

\begin{figure}
    \centering
    \caption{Gráfico da anomalia (°C) em função do tempo (anos), 1970-2025, com modelo de regressão linear}
    \includegraphics[width=\linewidth]{static/R/plot/anom-3-scatter+reg.png}
    \label{fig:2:anom-3-scatter+reg}
\end{figure}

Antes, foi dito que $\beta_1$ representava a inclinação da reta.
Além disso, ele cumpre uma outra função importante: representar a \textit{taxa de variação} da reta; como a unidade de medida em questão é anos, isso significa quanto a temperatura vai variar por ano.
A regressão realizada nesse conjunto de dados revela que a anomalia de temperatura média global cresce em um ritmo de $0,01879 °C$ ao ano.

Perceptivelmente, essa anomalia vem crescendo mais nos últimos anos do que vinha anteriormente, devido às mudanças climáticas.
O modelo de reta, por ter que simplificar drasticamente, é incapaz de expressar isso, de tal modo que nos últimos três anos sua estimativa está abaixo do valor real.
Geralmente, o que importa não é o quão próximo o modelo está dos dados já existentes, mas sim o quão bem o modelo se dá com extrapolar esses dados e, literalmente, prever o futuro.
Dadas essas tendências recentes, esse modelo pode não ser tão útil, uma vez que não detêm a habilidade de contemplá-las suficientemente.
Um modelo melhor, nesse caso, ao invés de uma reta usaria uma curva.
Um exemplo está na figura \ref{fig:2:anom-3-scatter+reg+poly}.

\begin{figure}
    \centering
    \caption{Gráfico da anomalia (°C) em função do tempo (anos), 1970-2025, com regressão linear e aproximação polinomial}
    \includegraphics[width=\linewidth]{static/R/plot/anom-5-scatter+reg+poly.png}
    \label{fig:2:anom-3-scatter+reg+poly}
\end{figure}

Esses modelo não é mais linear, evidentemente.
É, pelo contrário, uma curva definida por meio de um polinômio.
Nesse caso, a representação matemática está mais aquém de

\begin{equation}
A(t) = \beta_0 + t \beta_1 + t^2 \beta_2 + t^3 \beta_3 \dots t^n \beta_n 
\end{equation}

Onde $n$ é o grau do polinômio a ser criado.
Aqui, não se trata de uma regressão \textit{linear}, mas sim, \textit{polinomial}.

Está além do escopo deste presente texto debater o funcionamento interno desses modelos (isto é, como efetivamente ocorre a regressão linear), mas é importante perceber que, quando se utiliza uma curva, ao invés de uma reta, o modelo torna-se mais flexível, podendo adaptar-se melhor às condições dos dados.
O erro, que debatemos antes, é menor.
Contudo, ao invés de dois parâmetros simples -- $\beta_0$ e $\beta_1$ -- agora há $n$, que não podem ser interpretados de uma maneira tão direta quanto esses dois.
Isto é, ganha-se flexibilidade, mas há uma perda de interpretabilidade.
Na próxima seção, \ref{sec:2:redes-neurais}, será estudado um modelo ainda mais flexível; e ainda menos interpretável
A seção \ref{sec:2:exemplo-real} debaterá como essa ``interpretabilidade'' é refletida em um caso real: é possível pedir explicações desses modelos?
Isto é, determinar porque algo foi classificado como bom ou ruim, A ou B?

O exemplo aqui apresentado parece simples, mas serve para definir um conceito importante, que será retomado múltiplas vezes: o \textit{aprendizado de máquina}.
Para um computador, aprender é diminuir uma função erro, por meio da alteração de um conjunto de parâmetros.
O aprendizado requer dados; muitos dados; e aí entra o conteúdo do primeiro capítulo.
Terminado o treinamento, o modelo vai poder, a partir dos dados coletados, realizar previsões, inferências, classificações, entre outros, acerca desses dados.

Regressões lineares são um dos algoritmos mais simples que demonstram a dinâmica completa do aprendizado de máquina; tanto é, que foi o primeiro desses algoritmos a ser estudado, com o método de mínimos quadrados tendo sido empregado por Isaac Newton para determinar a duração exata do ano \cite{belenkiy_groping_2016}.
Há diversas maneiras de melhorá-lo.
Uma das mais simples é realizar a regressão em mais de uma variável; por exemplo, podemos estimar o preço de uma casa baseado no tamanho (em $\text{m}^2$), o número de quartos, a distância do centro, o andar, e a idade da casa.
O modelo se assemelharia à equação \ref{eq:2:lm-mult}.

\begin{equation}
\label{eq:2:lm-mult}
\text{Preço} = \beta_0 + \beta_1 \cdot (\text{Tamanho}) + \beta_2 \cdot (\text{Quartos}) + \beta_3\cdot (\text{Distância}) + \beta_5 \cdot (\text{Andar}) + \beta_6 \cdot (\text{Idade})
\end{equation}

Onde $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, $\beta_5$ e $\beta_6$ representam o quão significativos seus respectivos parâmetros são para o preço total do apartamento.

\section{Redes Neurais}\label{sec:2:redes-neurais}
Por mais úteis que as regressões lineares possam ser, ainda são um modelo rudimentar, que trabalha com alguns pressupostos bastante restritivos.
Um deles é o da linearidade da relação entre uma variável -- como o ano, no caso da anomalia de temperatura, ou o andar, no caso do preço de apartamentos -- e o resultado desejado.
Assim, pressupõe-se que essa relação se dá por uma reta, e não, por exemplo, um curva exponencial, logarítmica, ou qualquer outra coisa que fuja da linearidade.
Viu-se que isso pode ser consertado por meio de uma regressão polinomial\footnote{A regressão polinomial está associada à muitos outros métodos que a acompanham. Para estendê-la para uma predição com base em multiplas variáveis, como na equação \ref{eq:2:lm-mult}, por exemplo, é possível usar um Método Aditivo Generalizado (GAM). Os GAMs estabelescem uma função polinomial de grau $n$ \textit{para cada variável}, ou seja, uma função para o tamanho, uma para o número de quartos, uma para a distância do centro, e por aí vai. Assim, o modelo toma a cara de $\text{Preço} = \beta_0 + f_1(X_1) + f_2(X_2) + f_3(X_3) + \dots + f_i(X_i)$, onde $X_1\dots X_i$ são as variáveis (como o andar e o número de quartos) e $f_1\dots f_i$ são as funções de grau $n$ que dependem de cada variável \cite{james_introduction_2021a}}.
Contudo, há ainda um problema maior que impede-a de ser utilizada em mais contextos: a possiblidade de relações entre variáveis.
Para entender o que isso significa, considere um outro exemplo: o banco de dados MNIST \cite{lecun_mnist_}.

Ele é composto por milhares de imagens de dígitos de 0-9 desenhados à mão, assim como na figura \ref{fig:2:mnist}.
O objetivo que ele apresenta é fazer um modelo computacional capaz de identificar os dígitos baseando-se nas imagens apresentadas, semelhante ao que um ser humano faria ao ler esses números.
Por mais que isso pareça simples, esse problema não o é -- as imagens variam muito em sua forma, e a computação demorou para desenvolver métodos flexíveis o suficiente para essa tarefa.

\begin{figure}[H]
\caption{Algumas imagens do banco de dados MNIST}
\centering
\includegraphics[width=0.5\linewidth]{static/MNIST.png}
\label{fig:2:mnist}
\end{figure}

O banco de dados MNIST foi publicado em 1998, e, segundo o próprio autor, o recorde é de $0,23\%$ de erro, atingido em 2012, dentre os resultados publicados na literatura \cite{cireşan_multicolumn_2012}.
As primeiras tentativas resultaram em taxas por volta dos $12\%$ \cite{lecun_mnist_1998}.
\section{O Mundo Real}\label{sec:2:exemplo-real}
\section{Natureza}
