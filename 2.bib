@online{armltd_what_,
  title = {What Is {{Instruction Set Architecture}} ({{ISA}})?},
  author = {{Arm Ltd}},
  url = {https://www.arm.com/glossary/isa},
  urldate = {2025-08-16},
  abstract = {An Instruction Set Architecture (ISA) is part of the abstract model of a computer that defines how the CPU is controlled by the software. The ISA acts as an interface between the hardware and the soft},
  langid = {english},
  organization = {Arm | The Architecture for the Digital World},
  file = {/home/cisco/Zotero/storage/EIB4LUBI/isa.html}
}

@online{belenkiy_groping_2016,
  title = {Groping {{Toward Linear Regression Analysis}}: {{Newton}}'s {{Analysis}} of {{Hipparchus}}' {{Equinox Observations}}},
  shorttitle = {Groping {{Toward Linear Regression Analysis}}},
  author = {Belenkiy, Ari and Echague, Eduardo Vila},
  date = {2016-02-04},
  eprint = {0810.4948},
  eprinttype = {arXiv},
  eprintclass = {physics},
  doi = {10.48550/arXiv.0810.4948},
  url = {http://arxiv.org/abs/0810.4948},
  urldate = {2025-08-20},
  abstract = {In February 1700, Isaac Newton needed a precise tropical year to design a new universal calendar that would supersede the Gregorian one. However, 17th-Century astronomers were uncertain of the long-term variation in the inclination of the Earth's axis and were suspicious of Ptolemy's equinox observations. As a result, they produced a wide range of tropical years. Facing this problem, Newton attempted to compute the length of the year on his own, using the ancient equinox observations reported by a famous Greek astronomer Hipparchus of Rhodes, ten in number. Though Newton had a very thin sample of data, he obtained a tropical year only a few seconds longer than the correct length. The reason lies in Newton's application of a technique similar to modern regression analysis. Newton wrote down the first of the two so-called 'normal equations' known from the ordinary least-squares (OLS) method. In that procedure, Newton seems to have been the first to employ the mean (average) value of the data set, while the other leading astronomers of the era (Tycho Brahe, Galileo, and Kepler) used the median. Fifty years after Newton, in 1750, Newton's method was rediscovered and enhanced by Tobias Mayer. Remarkably, the same regression method served with distinction in the late 1920s when the founding fathers of modern cosmology, Georges Lemaitre (1927), Edwin Hubble (1929), and Willem de Sitter (1930), employed it to derive the Hubble constant.},
  pubstate = {prepublished},
  keywords = {Astrophysics,Physics - History and Philosophy of Physics},
  file = {/home/cisco/Zotero/storage/JLHRZIGF/Belenkiy and Echague - 2016 - Groping Toward Linear Regression Analysis Newton's Analysis of Hipparchus' Equinox Observations.pdf;/home/cisco/Zotero/storage/U9JH9SYY/0810.html}
}

@online{cireşan_multicolumn_2012,
  title = {Multi-Column {{Deep Neural Networks}} for {{Image Classification}}},
  author = {Cireşan, Dan and Meier, Ueli and Schmidhuber, Juergen},
  date = {2012-02-13},
  eprint = {1202.2745},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1202.2745},
  url = {http://arxiv.org/abs/1202.2745},
  urldate = {2025-08-21},
  abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/cisco/Zotero/storage/8HZDSFXH/Cireşan et al. - 2012 - Multi-column Deep Neural Networks for Image Classification.pdf}
}

@inproceedings{cun_reading_1997,
  title = {Reading Checks with Multilayer Graph Transformer Networks},
  booktitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Cun, Yann Le and Bottou, L. and Bengio, Y.},
  date = {1997-04},
  volume = {1},
  pages = {151-154 vol.1},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.1997.599580},
  url = {https://ieeexplore.ieee.org/document/599580},
  urldate = {2025-08-21},
  abstract = {We propose a new machine learning paradigm called multilayer graph transformer network that extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as input and produce graphs as output. A complete check reading system based on this concept is described. The system combines convolutional neural network character recognizers with graph-based stochastic models trained cooperatively at the document level. It is deployed commercially and reads million of business and personal checks per month with record accuracy.},
  eventtitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  keywords = {Character recognition,Computer networks,Convolution,Equations,Handwriting recognition,Jacobian matrices,Multi-layer neural network,Neural networks,Nonhomogeneous media,Recurrent neural networks},
  file = {/home/cisco/Zotero/storage/BHZRQ5CI/599580.html}
}

@book{james_introduction_2021a,
  title = {An {{Introduction}} to {{Statistical Learning}}: With {{Applications}} in {{R}}},
  shorttitle = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2021},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer US},
  location = {New York, NY},
  doi = {10.1007/978-1-0716-1418-1},
  url = {https://link.springer.com/10.1007/978-1-0716-1418-1},
  urldate = {2025-08-09},
  isbn = {978-1-0716-1417-4 978-1-0716-1418-1},
  langid = {english},
  keywords = {data mining,inference,R,R software,statistical learning,supervised learning,unsupervised learning}
}

@online{lecun_mnist_1998,
  title = {{{MNIST}} Handwritten Digit Database, {{Yann LeCun}}, {{Corinna Cortes}} and {{Chris Burges}}},
  author = {LeCun, Yann and Cortes, Corinna and Burges, Cristopher J. C.},
  date = {1998},
  url = {https://yann.lecun.org/exdb/mnist/index.html},
  urldate = {2025-08-21},
  file = {/home/cisco/Zotero/storage/97V452G4/index.html}
}

@inproceedings{varoquaux_hype_2025,
  title = {Hype, {{Sustainability}}, and the {{Price}} of the {{Bigger-is-Better Paradigm}} in {{AI}}},
  booktitle = {Proceedings of the 2025 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Varoquaux, Gael and Luccioni, Sasha and Whittaker, Meredith},
  date = {2025-06-23},
  series = {{{FAccT}} '25},
  pages = {61--75},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3715275.3732006},
  url = {https://dl.acm.org/doi/10.1145/3715275.3732006},
  urldate = {2025-08-09},
  abstract = {With the growing attention and investment in recent AI approaches such as large language models, the narrative that the larger the AI system the more valuable, powerful and interesting it is is increasingly seen as common sense. But what is this assumption based on, and how are we measuring value, power, and performance? And what are the collateral consequences of this race to ever-increasing scale? Here, we scrutinize the current scaling trends and trade-offs across multiple axes and refute two common assumptions underlying the ‘bigger-is-better’ AI paradigm: 1) that performance improvements are driven by increased scale, and 2) that all interesting problems addressed by AI require large-scale models. Rather, we argue that this approach is not only fragile scientifically, but comes with undesirable consequences. First, it is not sustainable, as, despite efficiency improvements, its compute demands increase faster than model performance, leading to unreasonable economic requirements and a disproportionate environmental footprint. Second, it implies focusing on certain problems at the expense of others, leaving aside important applications, e.g. health, education, or the climate. Finally, it exacerbates a concentration of power, which centralizes decision-making in the hands of a few actors while threatening to disempower others in the context of shaping both AI research and its applications throughout society.},
  isbn = {979-8-4007-1482-5},
  file = {/home/cisco/Zotero/storage/4HASK3EM/Varoquaux et al. - 2025 - Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI.pdf}
}
